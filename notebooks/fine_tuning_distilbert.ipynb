{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobJ215/Sentiment-Analysis-with-DistilBERT/blob/main/fine_tuning_distilbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GAaoj8X1cN9"
      },
      "source": [
        "# Fine Tuning DistilBERT - Amazon Polarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pb6lmwN1cN-"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOjtT_IX1cN_"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece] -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer, DataCollatorWithPadding"
      ],
      "metadata": {
        "id": "UWGcD754lMG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a subset of the 'amazon_polarity'\n",
        "amazon_train = load_dataset('amazon_polarity', split='train[:20000]')\n",
        "amazon_test = load_dataset('amazon_polarity', split='test[:2000]')\n",
        "\n",
        "print(\"Train Dataset : \", amazon_train.shape)\n",
        "print(\"Test Dataset : \", amazon_test.shape)"
      ],
      "metadata": {
        "id": "j3oqMrg_1d_t",
        "outputId": "23352bc2-a963-42cd-b329-742b5cf09742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset :  (20000, 3)\n",
            "Test Dataset :  (2000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_train"
      ],
      "metadata": {
        "id": "Be0BW07N1gXD",
        "outputId": "9c7c6100-de0a-4007-eaaf-3563e7cce8e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['label', 'title', 'content'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer, DataCollatorWithPadding\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "id": "5gcRle1G3IDU",
        "outputId": "eae4d84b-70d7-426c-fae0-31e7c6f9fbd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "def preprocess_data(data):\n",
        "    inputs = tokenizer(data['content'], truncation=True)\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "Jxe_RyFz3JlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text\n",
        "tokenized_datasets = amazon_train.map(preprocess_data, batched=True)\n",
        "tokenized_test_datasets = amazon_test.map(preprocess_data, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "# Create datasets\n",
        "tf_train_dataset = tokenized_datasets.to_tf_dataset(\n",
        "    columns=['input_ids', 'attention_mask', 'label'],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = tokenized_test_datasets.to_tf_dataset(\n",
        "    columns=['input_ids', 'attention_mask', 'label'],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n"
      ],
      "metadata": {
        "id": "LdBI-uVk3LTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115f9d33-ea69-4754-db41-73aefb2d5975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create learning rate scheduler\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "\n",
        "lr_scheduler = PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=lr_scheduler)\n",
        "\n"
      ],
      "metadata": {
        "id": "Otwfmj9P4SI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(tf_train_dataset,\n",
        "        validation_data=tf_validation_dataset,\n",
        "        epochs=2)"
      ],
      "metadata": {
        "id": "xgiY8rjq4T5z",
        "outputId": "4e2162a5-361d-4740-e083-d9454c06f287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "2500/2500 [==============================] - 566s 211ms/step - loss: 0.2744 - accuracy: 0.8867 - val_loss: 0.2380 - val_accuracy: 0.9030\n",
            "Epoch 2/2\n",
            "2500/2500 [==============================] - 440s 176ms/step - loss: 0.1119 - accuracy: 0.9615 - val_loss: 0.2537 - val_accuracy: 0.9075\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bb14dfa0190>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(tf_validation_dataset)[\"logits\"]\n",
        "class_preds = np.argmax(preds, axis=1)\n",
        "print(preds.shape, class_preds.shape)"
      ],
      "metadata": {
        "id": "0iKY281eABjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95fb52bb-896f-4025-d65b-fc92c514e6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 18s 62ms/step\n",
            "(2000, 2) (2000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Make prediction\n",
        "class_preds = np.argmax(model.predict(tf_validation_dataset)[\"logits\"], axis=1)\n",
        "\n",
        "# Retrieve the original sentences\n",
        "original_sentences = amazon_test[\"content\"]\n",
        "\n",
        "# Create lists to store positive and negative sentences\n",
        "positive_sentences = []\n",
        "negative_sentences = []\n",
        "\n",
        "for i, pred in enumerate(class_preds):\n",
        "    if pred == 1:\n",
        "        positive_sentences.append(original_sentences[i])\n",
        "    else:\n",
        "        negative_sentences.append(original_sentences[i])\n",
        "\n",
        "# Print some examples\n",
        "print(\"Positive Sentences:\")\n",
        "for i in range(5):  # Print first 5 positive sentences\n",
        "    print(positive_sentences[i])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "gNUkFdIl_znE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02044225-9112-4129-9d68-cde96bb7727b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 16s 63ms/step\n",
            "Positive Sentences:\n",
            "My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n",
            "\n",
            "\n",
            "Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, which I find distracting. But even if those weren't included I would still consider the collection worth it.\n",
            "\n",
            "\n",
            "Check out Maha Energy's website. Their Powerex MH-C204F charger works in 100 minutes for rapid charge, with option for slower charge (better for batteries). And they have 2200 mAh batteries.\n",
            "\n",
            "\n",
            "Reviewed quite a bit of the combo players and was hesitant due to unfavorable reviews and size of machines. I am weaning off my VHS collection, but don't want to replace them with DVD's. This unit is well built, easy to setup and resolution and special effects (no progressive scan for HDTV owners) suitable for many people looking for a versatile product.Cons- No universal remote.\n",
            "\n",
            "\n",
            "Exotic tales of the Orient from the 1930's. \"Dr Shen Fu\", a Weird Tales magazine reprint, is about the elixir of life that grants immortality at a price. If you're tired of modern authors who all sound alike, this is the antidote for you. Owen's palette is loaded with splashes of Chinese and Japanese colours. Marvelous.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Negative Sentences:\")\n",
        "for i in range(5):  # Print first 5 negative sentences\n",
        "    print(negative_sentences[i])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuqC1d5pihX4",
        "outputId": "5f570808-7998-4728-fa04-b78bdd9e3ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative Sentences:\n",
            "I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge. Might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n",
            "\n",
            "\n",
            "I also began having the incorrect disc problems that I've read about on here. The VCR still works, but hte DVD side is useless. I understand that DVD players sometimes just quit on you, but after not even one year? To me that's a sign on bad quality. I'm giving up JVC after this as well. I'm sticking to Sony or giving another brand a shot.\n",
            "\n",
            "\n",
            "I love the style of this, but after a couple years, the DVD is giving me problems. It doesn't even work anymore and I use my broken PS2 Now. I wouldn't recommend this, I'm just going to upgrade to a recorder now. I wish it would work but I guess i'm giving up on JVC. I really did like this one... before it stopped working. The dvd player gave me problems probably after a year of having it.\n",
            "\n",
            "\n",
            "I cannot scroll through a DVD menu that is set up vertically. The triangle keys will only select horizontally. So I cannot select anything on most DVD's besides play. No special features, no language select, nothing, just play.\n",
            "\n",
            "\n",
            "If you want to listen to El Duke , then it is better if you have access to his shower,this is not him, it is a gimmick,very well orchestrated.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(input_text):\n",
        "    # Preprocess input text\n",
        "    inputs = tokenizer(input_text, truncation=True, padding=True, return_tensors='tf')\n",
        "\n",
        "    # Get model prediction\n",
        "    logits = model(inputs)[\"logits\"]\n",
        "    predicted_class = np.argmax(logits, axis=1)[0]\n",
        "\n",
        "    # Determine sentiment label\n",
        "    sentiment_label = \"positive\" if predicted_class == 1 else \"negative\"\n",
        "\n",
        "    return sentiment_label\n",
        "\n",
        "# Example usage\n",
        "input_text = \"This is a horrible product it sucks\"\n",
        "predicted_sentiment = predict_sentiment(input_text)\n",
        "print(f\"The sentiment is {predicted_sentiment}\")"
      ],
      "metadata": {
        "id": "XwtEWNl_AS7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa10a46-37d8-48d1-a692-d584ab74407f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment is negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('fine_tuned_distilbert')"
      ],
      "metadata": {
        "id": "5_jXboFPAQle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/fine_tuned_distilbert.zip /content/fine_tuned_distilbert/\n",
        "files.download('/content/fine_tuned_distilbert.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "G5BXDYzEi6ff",
        "outputId": "0d7866ea-3787-4a6d-9269-470390b8c7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/fine_tuned_distilbert/ (stored 0%)\n",
            "  adding: content/fine_tuned_distilbert/tf_model.h5 (deflated 8%)\n",
            "  adding: content/fine_tuned_distilbert/config.json (deflated 44%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a73d4f93-3944-4f1c-b4c9-cd86cd8cdc20\", \"fine_tuned_distilbert.zip\", 246793172)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bK15zfyMjQKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}